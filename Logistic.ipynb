{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "import torch.tensor as T\n",
    "from torch.autograd import Variable as V\n",
    "import pickle\n",
    "import gzip\n",
    "import torch.optim\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and normalizing the MNIST dataset which will be used for benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL='http://deeplearning.net/data/mnist/'\n",
    "FILENAME='mnist.pkl.gz'\n",
    "\n",
    "def load_mnist(filename):\n",
    "    return pickle.load(gzip.open(filename, 'rb'), encoding='latin-1')\n",
    "path = 'data/mnist/'\n",
    "\n",
    "df = load_mnist(path+FILENAME)\n",
    "\n",
    "(x,y),(x_valid,y_valid),(x_test,y_test) = df\n",
    "mean = x.mean()\n",
    "std = x.std()\n",
    "\n",
    "x=(x-mean)/std\n",
    "\n",
    "x_valid = (x_valid-mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Logistic Regression network architecture using torch nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(*dims): return nn.Parameter(torch.randn(dims)/dims[0])\n",
    "def softmax(x): return torch.exp(x)/(torch.exp(x).sum(dim=1)[:,None])\n",
    "\n",
    "class LogReg_torch(nn.Module):\n",
    "    def __init__(self,dims,output):\n",
    "        super().__init__()\n",
    "        self.dims, self.output = dims, output\n",
    "        self.l1_w = get_weights(dims, output)  # Layer 1 weights\n",
    "        self.l1_b = get_weights(output)         # Layer 1 bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = (x @ self.l1_w) + self.l1_b  # Linear Layer\n",
    "        x = torch.log(softmax(x)) # Non-linear (LogSoftmax) Layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a class for logistic regression (dataloader, fit and predict) with an API similar to SKLearn API (Also supports GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegresssion_T():\n",
    "    def __init__(self,epochs = 3,batch_size='default', lr = 3e-3, weight_decay = 0, gpu = 0):\n",
    "        self.epochs,self.batch_size = epochs, batch_size\n",
    "        self.device = \"cuda\" if gpu ==1  else \"cpu\"\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "    def dataloader_gpu(self,x,y):\n",
    "        x_t = T(x); y_t = T(y)\n",
    "        dataset = tud.TensorDataset(x_t.to(self.device),y_t.to(self.device))\n",
    "        dl = tud.DataLoader(dataset,batch_size = self.batch_size)\n",
    "        return dl\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        self.batch_size = int(x.shape[0]/210) if self.batch_size == 'default' else self.batch_size \n",
    "        dims = x.shape[1]; output = len(set(y))\n",
    "        trn_dl = self.dataloader_gpu(x,y)\n",
    "        loss = nn.NLLLoss()\n",
    "        self.net = LogReg_torch(dims,output).to(self.device)\n",
    "#         optimizer =torch.optim.SGD(self.net.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        optimizer = torch.optim.Adam(self.net.parameters(), lr = self.lr, weight_decay = self.weight_decay)\n",
    "        trn_dl = self.dataloader_gpu(x,y)\n",
    "        for epoch in range(self.epochs):\n",
    "            losses=[]\n",
    "            dl = iter(trn_dl)\n",
    "            for t in range(len(dl)):\n",
    "                xt, yt = next(dl)\n",
    "                y_pred = self.net(V(xt).to(self.device))\n",
    "                l = loss(y_pred, V(yt).to(self.device))\n",
    "                losses.append(l)\n",
    "                optimizer.zero_grad()        \n",
    "                l.backward()       \n",
    "                optimizer.step()\n",
    "#             accuracy = accuracy_score(self.predict(x),y)\n",
    "#             print(\"epoch: \",epoch,  \"\\t loss: \", l.item(), \"\\t accuracy: \", accuracy)\n",
    "    def predict(self,x): return self.net(T(x).to(self.device)).detach().cpu().numpy().argmax(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Performance vs. SKLearn Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "TorchLog_cpu = LogisticRegresssion_T()\n",
    "SKLog_cpu = LogisticRegression(multi_class='auto', solver= 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.46 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9268"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time TorchLog_cpu.fit(x,y)\n",
    "accuracy_score(y_valid,TorchLog_cpu.predict(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9266"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time SKLog_cpu.fit(x,y)\n",
    "accuracy_score(y_valid,SKLog_cpu.predict(x_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Torch Version of Logistic Regression achieved the same accuracy as sklearn Logistic Regression (92.68% vs 92.66%), while the performance of the Torch Version is better with more than 100% (4.46s vs 10.3s), I have tried to use n_jobs in sklearn version but it somehow gives worse results. Torch version also provides the option to use GPU which will yield much better results on bigger datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
